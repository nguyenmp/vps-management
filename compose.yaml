
---

services:
  reverse-proxy:
    # The official v3 Traefik docker image
    image: traefik
    # Enables the web UI and tells Traefik to listen to docker
    command:
      - "--api=true"
      - "--api.insecure=true"
      - "--providers.docker"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.web.http.redirections.entryPoint.to=websecure"
      - "--entrypoints.web.http.redirections.entryPoint.scheme=https"
      - "--entrypoints.web.http.redirections.entrypoint.permanent=true"
      - "--certificatesresolvers.myresolver.acme.httpchallenge.entrypoint=web"
      - "--certificatesresolvers.myresolver.acme.email=nguyenmp605@gmail.com"
      - "--certificatesresolvers.myresolver.acme.storage=/letsencrypt/acme.json"
    ports:
      # The HTTP port
      - "80:80"
      # The HTTP port
      - "443:443"
      # The Web UI (enabled by --api.insecure=true)
      - "8080:8080"
    volumes:
      # So that Traefik can listen to the Docker events
      - /var/run/docker.sock:/var/run/docker.sock
      - ./letsencrypt-data/:/letsencrypt/
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "http://localhost:8080", "-O", "-"]
      interval: 60s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
  hikariita:
    build: https://github.com/nguyenmp/hikariita.git
    volumes:
      - ${HIKARIITA_DB_FILE:?error}:/usr/src/app/example.db
    restart: unless-stopped
    labels:
      - "traefik.http.routers.hikariita.rule=Host(`hikariita.docker.localhost`) || Host(`hikariita.href.cat`)"
      - traefik.http.routers.hikariita.tls=true
      - traefik.http.routers.hikariita.tls.certresolver=myresolver
      - traefik.http.routers.hikariita.tls.domains[0].main=hikariita.href.cat
      - traefik.http.services.hikariita.loadbalancer.server.port=80
    healthcheck:
      test: ["CMD", "curl", "http://localhost:80/"]
      interval: 61s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
  nginx:
    image: nginx
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./www/:/etc/nginx/www/:ro
    environment:
      - NGINX_PORT=80
    restart: unless-stopped
    labels:
      - "traefik.http.routers.static.rule=Host(`static.docker.localhost`) || Host(`static.href.cat`)"
      - traefik.http.routers.static.tls=true
      - traefik.http.routers.static.tls.certresolver=myresolver
      - traefik.http.routers.static.tls.domains[0].main=static.href.cat
      - traefik.http.services.static.loadbalancer.server.port=80
    healthcheck:
      test: ["CMD", "curl", "http://localhost:80/"]
      interval: 62s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
  changedetection:
    image: ghcr.io/dgtlmoon/changedetection.io
    environment:
      - BASE_URL=changes.href.cat
      # Alternative target "Chrome" Playwright URL, do not use "'s or 's!
      # "Playwright" is a driver/librarythat allows changedetection to talk to a Chrome or similar browser.
      - PLAYWRIGHT_DRIVER_URL=ws://sockpuppetbrowser:3000
    volumes:
      - ./changedetection-data/:/datastore/
    restart: unless-stopped
    labels:
      - "traefik.http.routers.changes.rule=Host(`changes.docker.localhost`) || Host(`changes.href.cat`)"
      - traefik.http.routers.changes.tls=true
      - traefik.http.routers.changes.tls.certresolver=myresolver
      - traefik.http.routers.changes.tls.domains[0].main=changes.href.cat
      - traefik.http.services.changes.loadbalancer.server.port=5000
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import requests; requests.get('http://localhost:5000').raise_for_status()\""]
      interval: 63s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
    # If WEBDRIVER or PLAYWRIGHT are enabled, changedetection container depends on that
    # and must wait before starting (substitute "browser-chrome" with "playwright-chrome" if last one is used)
    depends_on:
      sockpuppetbrowser:
        condition: service_started
  # Sockpuppetbrowser is basically chrome wrapped in an API for allowing fast fetching of web-pages.
  # RECOMMENDED FOR FETCHING PAGES WITH CHROME
  sockpuppetbrowser:
    hostname: sockpuppetbrowser
    image: dgtlmoon/sockpuppetbrowser:latest
    cap_add:
      - SYS_ADMIN
# SYS_ADMIN might be too much, but it can be needed on your platform https://github.com/puppeteer/puppeteer/blob/main/docs/troubleshooting.md#running-puppeteer-on-gitlabci
    restart: unless-stopped
    environment:
      - SCREEN_WIDTH=1920
      - SCREEN_HEIGHT=1024
      - SCREEN_DEPTH=16
      - MAX_CONCURRENT_CHROME_PROCESSES=10
    labels:
      - traefik.enable=false
  archivebox:
    image: markerz/archivebox:latest
    volumes:
      - ./archivebox-data/:/data/
      - ${YT_DLP_COOKIES_FILE:?error}:/data/yt-dlp-cache/cookies.txt
      # ./data/personas/Default/chrome_profile/Default:/data/personas/Default/chrome_profile/Default
    environment:
      # - ADMIN_USERNAME=admin            # create an admin user on first run with the given user/pass combo
      # - ADMIN_PASSWORD=SomeSecretPassword
      - CSRF_TRUSTED_ORIGINS=https://archivebox.href.cat,https://archivebox.docker.localhost  # REQUIRED for auth, REST API, etc. to work
      - ALLOWED_HOSTS=*                   # set this to the hostname(s) from your CSRF_TRUSTED_ORIGINS
      - PUBLIC_INDEX=True                 # set to False to prevent anonymous users from viewing snapshot list
      - PUBLIC_SNAPSHOTS=True             # set to False to prevent anonymous users from viewing snapshot content
      - PUBLIC_ADD_VIEW=False             # set to True to allow anonymous users to submit new URLs to archive
      - SEARCH_BACKEND_ENGINE=sonic       # tells ArchiveBox to use sonic container below for fast full-text search
      - SEARCH_BACKEND_HOST_NAME=sonic
      - SEARCH_BACKEND_PASSWORD=${SEARCH_BACKEND_PASSWORD:?error}
      # - PUID=911                        # set to your host user's UID & GID if you encounter permissions issues
      # - PGID=911                        # UID/GIDs <500 may clash with existing users and are not recommended
      # - MEDIA_MAX_SIZE=750m             # increase this filesize limit to allow archiving larger audio/video files
      # - TIMEOUT=60                      # increase this number to 120+ seconds if you see many slow downloads timing out
      # - CHECK_SSL_VALIDITY=True         # set to False to disable strict SSL checking (allows saving URLs w/ broken certs)
      # - SAVE_ARCHIVE_DOT_ORG=True       # set to False to disable submitting all URLs to Archive.org when archiving
      # - USER_AGENT="..."                # set a custom USER_AGENT to avoid being blocked as a bot
      # ...
      # add further configuration options from archivebox/config.py as needed (to apply them only to this container)
      # or set using `docker compose run archivebox config --set SOME_KEY=someval` (to persist config across all containers)
    # For ad-blocking during archiving, uncomment this section and pihole service section below
    # networks:
    #   - dns
    # dns:
    #   - 172.20.0.53
    restart: unless-stopped
    labels:
      - "traefik.http.routers.archivebox.rule=Host(`archivebox.docker.localhost`) || Host(`archivebox.href.cat`)"
      - traefik.http.routers.archivebox.tls=true
      - traefik.http.routers.archivebox.tls.certresolver=myresolver
      - traefik.http.routers.archivebox.tls.domains[0].main=archivebox.href.cat
      - traefik.http.services.archivebox.loadbalancer.server.port=8000
    healthcheck:
      test: ["CMD", "curl", "http://localhost:8000/"]
      interval: 64s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
    platform: linux/amd64
  tor-socks-proxy:
    image: peterdavehello/tor-socks-proxy
    expose:
      - 9150
    restart: unless-stopped
    platform: linux/amd64
    healthcheck:
      test: ["CMD", "curl", "icanhazip.com", "-x", "socks5://localhost:9150"]
      interval: 65s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
  sonic:
    image: valeriansaliou/sonic:latest
    expose:
      - 1491
    environment:
      - SEARCH_BACKEND_PASSWORD=${SEARCH_BACKEND_PASSWORD:?error}
    volumes:
      - ./sonic.cfg:/etc/sonic.cfg:ro    # use this if you prefer to download the config on the host and mount it manually
      - ./sonic-data/sonic:/var/lib/sonic/store
    restart: unless-stopped
    platform: linux/amd64
  postgres-recipes:
    image: pgvector/pgvector:pg${POSTGRES_VERSION:?error}
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:?error}
    ports:
      - 5432:5432
    volumes:
      - ./postgres-recipes-data/:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      interval: 67s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
  pgbackups3:
    build:
      context: https://github.com/miharekar/postgres-backup-s3.git
      args:
        POSTGRES_VERSION: ${POSTGRES_VERSION:?error}
    links:
      - postgres-recipes
    depends_on:
      postgres-recipes:
        condition: service_healthy
    restart: unless-stopped
    environment:
      SCHEDULE: '@daily'
      # S3_REGION: region
      S3_ACCESS_KEY_ID: ${POSTGRES_BACKUP_S3_ACCESS_KEY_ID:?error}
      S3_SECRET_ACCESS_KEY: ${POSTGRES_BACKUP_S3_SECRET_ACCESS_KEY:?error}
      S3_BUCKET: recipes.backup.production
      S3_PREFIX: backup_${STAGE:?error}
      POSTGRES_DATABASE: postgres
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?error}
      POSTGRES_EXTRA_OPTS: '--schema=public --blobs'
      POSTGRES_HOST: postgres-recipes
  recipes:
    image: markerz/recipes:latest
    env_file: ${RECIPES_ENV_FILE:?error}
    restart: unless-stopped
    labels:
      - "traefik.http.routers.recipes.rule=Host(`recipes.docker.localhost`) || Host(`recipes.href.cat`)"
      - traefik.http.routers.recipes.tls=true
      - traefik.http.routers.recipes.tls.certresolver=myresolver
      - traefik.http.routers.recipes.tls.domains[0].main=recipes.href.cat
      - traefik.http.services.recipes.loadbalancer.server.port=3000
    depends_on:
      postgres-recipes:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "http://localhost:3000/recipes"]
      interval: 68s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5
    platform: linux/amd64

  lowcoder-all-in-one:
    image: lowcoderorg/lowcoder-ce:latest
    expose:
      - 3000
      - 3443
    env_file:
      - envs/lowcoder-default.env
    restart: unless-stopped
    volumes:
      - ./lowcoder-data/stacks/:/lowcoder-stacks
      - ./lowcoder-data/assets/:/lowcoder/assets
    labels:
      - "traefik.http.routers.lowcoder.rule=Host(`lowcoder.docker.localhost`) || Host(`lowcoder.href.cat`)"
      - traefik.http.routers.lowcoder.tls=true
      - traefik.http.routers.lowcoder.tls.certresolver=myresolver
      - traefik.http.routers.lowcoder.tls.domains[0].main=lowcoder.href.cat
      - "traefik.http.services.lowcoder.loadbalancer.server.port=3000"
    healthcheck:
      test: ["CMD", "curl", "http://localhost:3000/api/status/health"]
      interval: 70s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5

  n8n:
    build: n8n/.
    environment:
      - N8N_RUNNERS_ENABLED=true
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      # https://docs.n8n.io/hosting/configuration/configuration-examples/webhook-url/
      - WEBHOOK_URL=${N8N_WEBHOOK_URL?error}

      # Trust the X-forwarded-for header from the reverse proxy
      # https://community.n8n.io/t/x-forwarded-for-header-is-set-but-the-express-trust-proxy-s/51208
      - N8N_PROXY_HOPS=1

      - GENERIC_TIMEZONE=America/Los_Angeles

      # This database was inited by:
      # exec-ing into the container
      # psql -h localhost -U postgres
      # CREATE DATABASE n8n;
      # then restore from n8n;;;
      # cd .n8n/
      # mkdir backup
      # https://docs.n8n.io/hosting/cli-commands/#export-entities
      # n8n export:workflow --backup --output=backup/workflows/
      # n8n export:credentials --backup --output=backup/credentials/
      # n8n export:entities --outputDir=./backup/ # --includeExecutionHistoryDataTables=true
      # manually reinstall community nodes and API keys
      # Reactivate license
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres-recipes
      - DB_POSTGRESDB_DATABASE=n8n
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_USER=postgres
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD:?error}
      - DB_POSTGRESDB_SCHEMA=public

      - EXECUTIONS_MODE=queue
      - N8N_CONCURRENCY_PRODUCTION_LIMIT=20
      - QUEUE_BULL_REDIS_HOST=redis
      - QUEUE_BULL_REDIS_PORT=6379

      - NODES_EXCLUDE="[]"
      - N8N_RESTRICT_FILE_ACCESS_TO="/home/node/.n8n-files/;/tmp/;/home/node/"


    expose:
      - 5678
    restart: unless-stopped
    volumes:
      - ./n8n-data/:/home/node/.n8n
    depends_on:
      postgres-recipes:
        condition: service_healthy
    labels:
      - "traefik.http.routers.n8n.rule=Host(`n8n.docker.localhost`) || Host(`n8n.href.cat`)"
      - traefik.http.routers.n8n.tls=true
      - traefik.http.routers.n8n.tls.certresolver=myresolver
      - traefik.http.routers.n8n.tls.domains[0].main=n8n.href.cat
      - "traefik.http.services.n8n.loadbalancer.server.port=5678"
    healthcheck:
      test: ["CMD", "wget", "http://localhost:5678/", "-O", "/dev/null"]
      interval: 71s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5

  n8n-worker:
    # docker run --name n8n-queue -p 5679:5678 docker.n8n.io/n8nio/n8n worker
    command: worker
    build: n8n/.
    environment:
      - N8N_RUNNERS_ENABLED=true
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      # https://docs.n8n.io/hosting/configuration/configuration-examples/webhook-url/
      - WEBHOOK_URL=${N8N_WEBHOOK_URL?error}

      # Trust the X-forwarded-for header from the reverse proxy
      # https://community.n8n.io/t/x-forwarded-for-header-is-set-but-the-express-trust-proxy-s/51208
      - N8N_PROXY_HOPS=1

      - GENERIC_TIMEZONE=America/Los_Angeles

      # This database was inited by:
      # exec-ing into the container
      # psql -h localhost -U postgres
      # CREATE DATABASE n8n;
      # then restore from n8n;;;
      # cd .n8n/
      # mkdir backup
      # https://docs.n8n.io/hosting/cli-commands/#export-entities
      # n8n export:workflow --backup --output=backup/workflows/
      # n8n export:credentials --backup --output=backup/credentials/
      # n8n export:entities --outputDir=./backup/ # --includeExecutionHistoryDataTables=true
      # manually reinstall community nodes and API keys
      # Reactivate license
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres-recipes
      - DB_POSTGRESDB_DATABASE=n8n
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_USER=postgres
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD:?error}
      - DB_POSTGRESDB_SCHEMA=public

      - EXECUTIONS_MODE=queue
      - N8N_CONCURRENCY_PRODUCTION_LIMIT=20
      - QUEUE_BULL_REDIS_HOST=redis
      - QUEUE_BULL_REDIS_PORT=6379
      - QUEUE_HEALTH_CHECK_ACTIVE=true

      - NODES_EXCLUDE="[]"
      - N8N_RESTRICT_FILE_ACCESS_TO="/home/node/.n8n-files/;/tmp/;/home/node/"


    expose:
      - 5678
    restart: unless-stopped
    volumes:
      - ./n8n-data/:/home/node/.n8n
    depends_on:
      postgres-recipes:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "http://localhost:5678/healthz/readiness", "-O", "/dev/null"]
      interval: 71s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5

  redis:
    # docker run --name some-redis -p 6379:6379  -d redis
    image: redis:latest
    expose:
      - 6379
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 72s
      timeout: 5s
      start_period: 30s
      start_interval: 5s
      retries: 5

  status-redirect:
    image: traefik/whoami
    restart: unless-stopped
    labels:
      - "traefik.http.routers.status.rule=Host(`status.href.cat`) || Host(`status.docker.localhost`)"
      - traefik.http.routers.status.tls=true
      - traefik.http.routers.status.tls.certresolver=myresolver
      - traefik.http.routers.status.tls.domains[0].main=status.href.cat
      - "traefik.http.routers.status.middlewares=status-redirect"
      - "traefik.http.middlewares.status-redirect.redirectregex.regex=^https?://(status\\.href\\.cat|status\\.docker\\.localhost)(.*)"
      - "traefik.http.middlewares.status-redirect.redirectregex.replacement=https://stats.uptimerobot.com/ZM36nymMTx$2"
      - "traefik.http.middlewares.status-redirect.redirectregex.permanent=false"
      - traefik.http.services.status.loadbalancer.server.port=80

  href-redirect:
    image: traefik/whoami
    restart: unless-stopped
    labels:
      - "traefik.http.routers.href.rule=Host(`href.cat`) || Host(`docker.localhost`)"
      - traefik.http.routers.href.tls=true
      - traefik.http.routers.href.tls.certresolver=myresolver
      - traefik.http.routers.href.tls.domains[0].main=href.cat
      - "traefik.http.routers.href.middlewares=href-redirect"
      - "traefik.http.middlewares.href-redirect.redirectregex.regex=^https?://(href\\.cat|docker\\.localhost)(.*)"
      - "traefik.http.middlewares.href-redirect.redirectregex.replacement=https://aggressivelyparaphrasing.me$2"
      - "traefik.http.middlewares.href-redirect.redirectregex.permanent=false"
      - traefik.http.services.href.loadbalancer.server.port=80

  telegraf:
    image: telegraf:latest
    restart: unless-stopped
    user: root
    ports:
      - 9273:9273
    entrypoint: "telegraf"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./telegraf.conf:/etc/telegraf/telegraf.conf:ro

  node_exporter:
    image: quay.io/prometheus/node-exporter:latest
    command:
      - '--path.rootfs=/host'
    network_mode: host
    restart: unless-stopped
    volumes:
      - /:/host:ro
    labels:
      - traefik.enable=false
  alloy:
    image: grafana/alloy:latest
    restart: unless-stopped
    network_mode: host
    environment:
      - ALLOY_LOG_LEVEL=info
      - GCLOUD_FM_COLLECTOR_ID=${GCLOUD_FM_COLLECTOR_ID:?error}
      - GCLOUD_RW_API_KEY=${GCLOUD_RW_API_KEY:?error}
      - DOCKER_HOSTNAME=${HOSTNAME:?error}
    volumes:
      - ./config.alloy:/etc/alloy/config.alloy:ro
    labels:
      - traefik.enable=false

  portainer:
    image: portainer/portainer-ce:lts
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./portainer-data/:/data
    labels:
      - "traefik.http.routers.portainer.rule=Host(`portainer.docker.localhost`) || Host(`portainer.href.cat`)"
      - traefik.http.routers.portainer.tls=true
      - traefik.http.routers.portainer.tls.certresolver=myresolver
      - traefik.http.routers.portainer.tls.domains[0].main=portainer.href.cat
      - "traefik.http.services.portainer.loadbalancer.server.port=9000"

  audiobookshelf:
    image: ghcr.io/advplyr/audiobookshelf:latest
    volumes:
      - ./audiobookshelf-data/audiobooks:/audiobooks
      - ./audiobookshelf-data/podcasts:/podcasts
      - ./audiobookshelf-data/config:/config
      - ./audiobookshelf-data/metadata:/metadata
    restart: unless-stopped
    environment:
      - TZ="America/Los_Angeles"
    labels:
      - "traefik.http.routers.audiobookshelf.rule=Host(`audiobookshelf.docker.localhost`) || Host(`audiobookshelf.href.cat`)"
      - traefik.http.routers.audiobookshelf.tls=true
      - traefik.http.routers.audiobookshelf.tls.certresolver=myresolver
      - traefik.http.routers.audiobookshelf.tls.domains[0].main=audiobookshelf.href.cat
      - "traefik.http.services.audiobookshelf.loadbalancer.server.port=80"

volumes:
  changedetection-data: